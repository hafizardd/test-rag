{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8d0b448",
   "metadata": {},
   "source": [
    "\n",
    "# CV Screening RAG Chatbot \n",
    "\n",
    "This notebook walks you through building a **Retrievalâ€‘Augmented Generation (RAG) chatbot** over a collection of resumes (CVs).  \n",
    "We will **manually upload CV in PDF** and build a private, local search system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db96bc3",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup & Imports\n",
    "\n",
    "Weâ€™ll use:\n",
    "- `PyPDF2` â†’ to extract text from CV (PDFs)\n",
    "- `tiktoken` â†’ for tokenization when chunking text\n",
    "- `faiss` â†’ local vector database to store embeddings\n",
    "- `groq api` â†’ for embeddings + chat completions\n",
    "- `dotenv` â†’ to load API keys from `.env`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "054aea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: '../requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17758e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Groq client ready\n",
      "ðŸ”§ Using model: openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from pypdf import PdfReader\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check API Key\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"âŒ GROQ_API_KEY not set. Please add it to your .env file.\")\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "print(\"âœ… Groq client ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fcf7e3",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Upload & Parse CV (PDFs)\n",
    "\n",
    "Weâ€™ll read CV from a local folder (`./uploads`).  \n",
    "Each PDF will be converted into plain text for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a257950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 0 resumes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "UPLOAD_DIR = \"uploads\"\n",
    "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "# Parse all resumes in uploads/\n",
    "resumes = {}\n",
    "for fname in os.listdir(UPLOAD_DIR):\n",
    "    if fname.lower().endswith(\".pdf\"):\n",
    "        path = os.path.join(UPLOAD_DIR, fname)\n",
    "        resumes[fname] = extract_text_from_pdf(path)\n",
    "\n",
    "print(f\"âœ… Loaded {len(resumes)} resumes\")\n",
    "list(resumes.keys())[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56471b",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Chunking\n",
    "\n",
    "Why chunking?  \n",
    "- pdf can be long. Embedding entire pdf leads to poor retrieval.  \n",
    "- We split text into **manageable chunks** (300â€“400 tokens) with slight overlap.\n",
    "\n",
    "This helps us retrieve only the most relevant pieces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab8338b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Example chunking on first resume\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m sample_resume = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresumes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m chunks = chunk_text(sample_resume)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst resume split into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import tiktoken\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 400, overlap: int = 60) -> List[str]:\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = enc.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens - overlap):\n",
    "        chunk = tokens[i:i+max_tokens]\n",
    "        chunks.append(enc.decode(chunk))\n",
    "    return chunks\n",
    "\n",
    "# Example chunking on first resume\n",
    "sample_resume = list(resumes.values())[0]\n",
    "chunks = chunk_text(sample_resume)\n",
    "print(f\"First resume split into {len(chunks)} chunks\")\n",
    "chunks[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5481c88",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Embeddings + FAISS Index\n",
    "\n",
    "Weâ€™ll convert each chunk into a **vector embedding** using OpenAI.  \n",
    "Then, weâ€™ll store all vectors in a **FAISS index** for fast similarity search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e8d613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 23\n",
      "âœ… FAISS index ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class DocChunk:\n",
    "    doc_id: str\n",
    "    chunk_id: int\n",
    "    text: str\n",
    "\n",
    "all_chunks: List[DocChunk] = []\n",
    "for doc_name, text in resumes.items():\n",
    "    chunks = chunk_text(text)\n",
    "    for i, ch in enumerate(chunks):\n",
    "        all_chunks.append(DocChunk(doc_id=doc_name, chunk_id=i, text=ch))\n",
    "\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n",
    "\n",
    "def embed_texts(texts: List[str]) -> np.ndarray:\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), 50):\n",
    "        batch = texts[i:i+50]\n",
    "        resp = client.embeddings.create(model=\"text-embedding-3-small\", input=batch)\n",
    "        vectors.extend([d.embedding for d in resp.data])\n",
    "        time.sleep(0.5)  # be polite\n",
    "    return np.array(vectors).astype(\"float32\")\n",
    "\n",
    "# Embed all chunks\n",
    "texts = [c.text for c in all_chunks]\n",
    "embeddings = embed_texts(texts)\n",
    "\n",
    "# Build FAISS index\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"âœ… FAISS index ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859422e0",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Retrieval + Answer Generation (RAG)\n",
    "\n",
    "Workflow:\n",
    "1. Embed the userâ€™s question\n",
    "2. Retrieve topâ€‘k most similar chunks\n",
    "3. Send them along with the question to OpenAI for a **grounded answer**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd7b14ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berdasarkan pengalaman kerja terlama, berikut adalah peringkat kandidat:\n",
      "\n",
      "1. **Michael Sihotang** - Pengalaman kerja sebagai Technical Product Ops Intern di Platform Indonesia dan Product Manager Intern di GDP Labs, dengan total pengalaman mencapai lebih dari 4 bulan dari November 2024 hingga sekarang. [CV - Michael Sihotang - Example.pdf#1].\n",
      "\n",
      "2. **Jessica Enaprilia Sihotang** - Memiliki pengalaman kerja sebagai Project Engineer Intern di PT Iforte Solusi Infotek dari Juli 2024 hingga September 2024, diikuti dengan posisi Operation Intern di Schoters dari Oktober 2024 hingga Januari 2025. Total pengalaman kerja adalah sekitar 4 bulan. [Resume - Jessica Enaprilia Sihotang - Hangry.pdf#0].\n",
      "\n",
      "3. **Priscilla Auleader Napitupulu** - Terlibat dalam beberapa kompetisi dan mendapatkan penghargaan sejak Maret 2023, tetapi tidak mencantumkan pengalaman kerja secara formal di dalam resume. Tidak ada informasi mengenai pengalaman kerja yang tertera. [CV - Priscilla Auleader Napitupulu.pdf#4].\n",
      "\n",
      "Jadi, peringkatnya adalah:\n",
      "1. Michael Sihotang\n",
      "2. Jessica Enaprilia Sihotang\n",
      "3. Priscilla Auleader Napitupulu (belum memiliki pengalaman kerja formal).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def search_index(query: str, k: int = 3):\n",
    "    q_emb = embed_texts([query])\n",
    "    D, I = index.search(q_emb, k)\n",
    "    return [(all_chunks[i], float(D[0][j])) for j, i in enumerate(I[0])]\n",
    "\n",
    "def answer_with_rag(query: str, k: int = 3) -> str:\n",
    "    results = search_index(query, k)\n",
    "    context = \"\\n---\\n\".join([f\"[{r.doc_id}#{r.chunk_id}] {r.text}\" for r, _ in results])\n",
    "    prompt = f\"\"\"You are an HR assistant.\n",
    "Use the following resume excerpts to answer the question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer clearly, citing resume IDs like [filename#chunk].\"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# Try a sample HR query\n",
    "print(answer_with_rag(\"Ranking kandidat berdasarkan pengalaman kerja terlama\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea4230",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Key Takeaways\n",
    "\n",
    "- RAG lets HR **search CV** using natural language (not just keywords).\n",
    "- **Chunking + embeddings** improves retrieval accuracy.\n",
    "- **FAISS** is fast and lightweight for local experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Next Steps\n",
    "- Add filters (e.g., years of experience, location).\n",
    "- Use a managed vector DB (Pinecone, Chroma, pgvector) for scale.\n",
    "- Improve PDF parsing (layout, tables, OCR).  \n",
    "- Add guardrails for bias + privacy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
